import streamlit as st
import asyncio
import pandas as pd
from datetime import datetime, timedelta
import sqlite3
import os
from universal_crawler import crawl_url_async, crawl_urls_async, crawl_list_page_async, get_supported_sources, universal_crawler
from db import get_all_companies, get_company_count, search_companies, get_companies_by_source, get_companies_by_date_range, get_latest_companies, clear_all_companies
from utils.logger import logger
from typing import List, Dict, Any

# Page config
st.set_page_config(
    page_title="Company Funding Crawler",
    page_icon="üí∞",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #c3e6cb;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #f5c6cb;
    }
    </style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=300)  # Cache data for 5 minutes instead of 1 hour
def get_database_stats():
    """Get database statistics with caching."""
    try:
        total_companies = get_company_count()
        latest_companies = get_latest_companies(5)
        return total_companies, latest_companies
    except Exception as e:
        logger.error(f"Error getting database stats: {e}")
        return 0, []

@st.cache_data(ttl=300)  # Cache data for 5 minutes instead of 30 minutes
def fetch_all_companies():
    """Fetch all companies from database with caching."""
    try:
        return get_all_companies()
    except Exception as e:
        logger.error(f"Error fetching companies: {e}")
        return []

def display_company_data(companies_data, show_save_button=True, save_to_db=False):
    """Display company data in a formatted table with optional save button."""
    if not companies_data:
        st.warning("No data to display")
        return
    
    # Convert to DataFrame for better display
    columns = [
        'raised_date', 'company_name', 'industry', 'ceo_name', 'procurement_name',
        'purchasing_name', 'manager_name', 'amount_raised', 'funding_round',
        'source', 'website', 'linkedin', 'article_url'
    ]
    
    df = pd.DataFrame(companies_data, columns=columns)
    
    # Display save button if requested
    if show_save_button and not save_to_db:
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.success(f"üìä **{len(companies_data)} records ready for review**")
        with col2:
            if st.button("üíæ Save to Database", type="primary", key=f"save_btn_{len(companies_data)}"):
                try:
                    from db import insert_many_companies
                    num_inserted = insert_many_companies(companies_data)
                    st.success(f"‚úÖ Successfully saved {num_inserted} records to database!")
                    
                    # Auto-refresh cache
                    st.info("üîÑ Refreshing data...")
                    st.cache_data.clear()
                    st.rerun()
                except Exception as e:
                    st.error(f"‚ùå Error saving to database: {str(e)}")
        with col3:
            if st.button("üîÑ Refresh", key=f"refresh_btn_{len(companies_data)}"):
                st.cache_data.clear()
                st.rerun()
    else:
        st.success(f"üìä **{len(companies_data)} records**")
    
    # Format the display
    st.dataframe(
        df,
        use_container_width=True,
        column_config={
            "raised_date": st.column_config.DateColumn("Published Date"),
            "company_name": st.column_config.TextColumn("Company Name", width="medium"),
            "industry": st.column_config.TextColumn("Industry", width="medium"),
            "ceo_name": st.column_config.TextColumn("CEO Name", width="medium"),
            "procurement_name": st.column_config.TextColumn("Procurement", width="medium"),
            "purchasing_name": st.column_config.TextColumn("Purchasing", width="medium"),
            "manager_name": st.column_config.TextColumn("Manager", width="medium"),
            "amount_raised": st.column_config.NumberColumn("Amount Raised", format="$%d"),
            "funding_round": st.column_config.TextColumn("Funding Round", width="medium"),
            "source": st.column_config.TextColumn("Source", width="small"),
            "website": st.column_config.LinkColumn("Website"),
            "linkedin": st.column_config.LinkColumn("LinkedIn"),
            "article_url": st.column_config.LinkColumn("Article URL")
        }
    )

def main():
    st.markdown('<h1 class="main-header">üí∞ Company Funding Crawler</h1>', unsafe_allow_html=True)
    
    # Sidebar navigation
    st.sidebar.title("Navigation")
    page = st.sidebar.selectbox(
        "Choose a page:",
        ["üè† Home", "ü§ñ Natural Language Crawler", "üöÄ AI Auto-Discovery", "üï∑Ô∏è Universal Crawler", "üìä Data View", "üîç Search & Filter", "‚öôÔ∏è Settings"]
    )
    
    if page == "üè† Home":
        show_dashboard()
    elif page == "ü§ñ Natural Language Crawler":
        show_natural_language_crawler()
    elif page == "üöÄ AI Auto-Discovery":
        show_ai_auto_discovery()
    elif page == "üï∑Ô∏è Universal Crawler":
        show_universal_crawler()
    elif page == "üìä Data View":
        show_data_view()
    elif page == "üîç Search & Filter":
        show_search_filter()
    elif page == "‚öôÔ∏è Settings":
        show_settings()

def show_dashboard():
    """Display the main dashboard."""
    st.header("üìä Home")
    
    # Add refresh button
    col1, col2 = st.columns([3, 1])
    with col1:
        st.markdown("### Dashboard")
    with col2:
        if st.button("üîÑ Refresh Data", help="Clear cache and refresh data"):
            st.cache_data.clear()
            st.rerun()
    
    # Get database statistics
    total_companies, latest_companies = get_database_stats()
    
    # Display metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Companies", total_companies)
    
    with col2:
        st.metric("Latest Update", datetime.now().strftime("%Y-%m-%d"))
    
    with col3:
        sources = get_supported_sources()
        st.metric("Supported Sources", len(sources))
    
    with col4:
        if latest_companies:
            latest_date = latest_companies[0][0] if latest_companies[0][0] else "N/A"
            st.metric("Latest Article", latest_date)
    
    # Display latest companies
    st.subheader("üìà Latest Companies")
    if latest_companies:
        display_company_data(latest_companies)
    else:
        st.info("No companies in database yet. Start crawling to see data here!")

def show_universal_crawler():
    """Display the universal crawler interface."""
    st.header("üï∑Ô∏è Universal Crawler")
    
    # Supported sources info with detailed breakdown
    sources = get_supported_sources()
    st.info(f"‚úÖ **{len(sources)} Supported Sources**")
    
    # Show sources in a more organized way
    col1, col2, col3 = st.columns(3)
    source_list = list(sources.values())

    with col1:
        st.markdown("**Major Tech News:**")
        for source in source_list[:7]:
            st.write(f"‚Ä¢ {source}")

    with col2:
        st.markdown("**Business & Finance:**")
        for source in source_list[7:14]:
            st.write(f"‚Ä¢ {source}")

    with col3:
        st.markdown("**Startup & VC:**")
        for source in source_list[14:]:
            st.write(f"‚Ä¢ {source}")

    st.info("üåê **Auto-Detection**: The system can also automatically detect and process other news sources based on domain patterns!")
    
    # List Page crawling
    st.markdown("### üì∞ List Page Crawling")
    list_page_url = st.text_input("List Page URL:", placeholder="https://techcrunch.com/startups/")
    
    # Date range filtering
    st.markdown("#### üìÖ Date Range Filter (Optional)")
    col1, col2 = st.columns(2)
    
    with col1:
        start_date = st.date_input("Start Date:", value=None)
    with col2:
        end_date = st.date_input("End Date:", value=None)
    
    # Save to database option
    save_to_db = st.checkbox("üíæ Auto Save to Database", value=False, help="N·∫øu b·ªè ch·ªçn, b·∫°n s·∫Ω c√≥ th·ªÉ xem x√©t d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u")
    
    if st.button("üöÄ Crawl List Page", type="primary"):
        if list_page_url:
            # Validate date range
            if (start_date and not end_date) or (end_date and not start_date):
                st.error("‚ùå Please select both start and end dates for date filtering")
            elif start_date and end_date and start_date > end_date:
                st.error("‚ùå Start date cannot be after end date")
            else:
                with st.spinner("Crawling list page..."):
                    try:
                        # Use default values: max_articles=20, num_workers=5
                        results = asyncio.run(crawl_list_page_async(
                            list_page_url, 20, 5, save_to_db,
                            start_date.strftime('%Y-%m-%d') if start_date else None,
                            end_date.strftime('%Y-%m-%d') if end_date else None
                        ))

                        if results:
                            st.success(f"‚úÖ Successfully processed {len(results)} articles!")
                            
                            # Display results summary
                            successful = [r for r in results if r.get('success')]
                            st.info(f"üìä Summary: {len(successful)} successful out of {len(results)} total")
                            
                            # Display results in table format
                            if successful:
                                # Convert to table format
                                table_data = []
                                for result in successful:
                                    table_data.append({
                                        'raised_date': result.get('raised_date'),
                                        'company_name': result.get('company_name'),
                                        'industry': result.get('industry'),
                                        'ceo_name': result.get('ceo_name'),
                                        'procurement_name': result.get('procurement_name'),
                                        'purchasing_name': result.get('purchasing_name'),
                                        'manager_name': result.get('manager_name'),
                                        'amount_raised': result.get('amount_raised'),
                                        'funding_round': result.get('funding_round'),
                                        'source': result.get('source'),
                                        'website': result.get('website'),
                                        'linkedin': result.get('linkedin'),
                                        'article_url': result.get('article_url')
                                    })
                                
                                st.success(f"üìä Displaying {len(table_data)} successful results:")
                                display_company_data(table_data, show_save_button=not save_to_db, save_to_db=save_to_db)
                            
                            # Show failed results in expander
                            failed_results = [r for r in results if not r.get('success')]
                            if failed_results:
                                with st.expander(f"‚ö†Ô∏è {len(failed_results)} Failed Results"):
                                    for i, result in enumerate(failed_results):
                                        st.error(f"**{i+1}. {result.get('url', 'Unknown URL')}**: {result.get('error')}")
                            
                            # Auto-refresh cache after successful crawl
                            if save_to_db and successful:
                                st.info("üîÑ Refreshing data...")
                                st.cache_data.clear()
                                st.rerun()
                        else:
                            st.warning("‚ö†Ô∏è No articles found or processed")
                    except Exception as e:
                        st.error(f"‚ùå Error: {str(e)}")
        else:
            st.warning("Please enter a list page URL")

def show_data_view():
    """Display the data view page."""
    st.header("üìä Data View")
    
    # Get all companies
    companies_data = fetch_all_companies()
    
    if companies_data:
        st.success(f"üìà Found {len(companies_data)} companies in database")
        display_company_data(companies_data, show_save_button=False, save_to_db=True)
        
        # Export options
        st.markdown("### üì§ Export Data")
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üìä Export to CSV"):
                df = pd.DataFrame(companies_data, columns=[
                    'raised_date', 'company_name', 'industry', 'ceo_name', 'procurement_name',
                    'purchasing_name', 'manager_name', 'amount_raised', 'funding_round',
                    'source', 'website', 'linkedin', 'article_url'
                ])
                csv = df.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name=f"companies_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        
        with col2:
            if st.button("üóëÔ∏è Clear All Data"):
                if st.button("‚ö†Ô∏è Confirm Clear All Data", type="secondary"):
                    if clear_all_companies():
                        st.success("‚úÖ All data cleared successfully!")
                        st.rerun()
            else:
                        st.error("‚ùå Failed to clear data")
    else:
        st.info("üì≠ No data in database. Start crawling to see data here!")

def show_search_filter():
    """Display the search and filter page."""
    st.header("üîç Search & Filter")
    
    # Search functionality
    st.markdown("### üîç Search Companies")
    search_query = st.text_input("Search by company name, industry, or CEO name:")
    
    if search_query:
        search_results = search_companies(search_query)
        if search_results:
            st.success(f"üîç Found {len(search_results)} matching companies")
            display_company_data(search_results, show_save_button=False, save_to_db=True)
        else:
            st.info("üîç No companies found matching your search")
    
    # Filter by source
    st.markdown("### üì∞ Filter by Source")
    sources = get_supported_sources()
    selected_source = st.selectbox("Select source:", ["All"] + list(sources.values()))
    
    if selected_source != "All":
        source_results = get_companies_by_source(selected_source)
        if source_results:
            st.success(f"üì∞ Found {len(source_results)} companies from {selected_source}")
            display_company_data(source_results, show_save_button=False, save_to_db=True)
    else:
            st.info(f"üì∞ No companies found from {selected_source}")
    
    # Filter by date range
    st.markdown("### üìÖ Filter by Date Range")
    col1, col2 = st.columns(2)
    
    with col1:
        filter_start_date = st.date_input("Start Date:", value=None)
    with col2:
        filter_end_date = st.date_input("End Date:", value=None)
    
    if filter_start_date and filter_end_date:
        if filter_start_date <= filter_end_date:
            date_results = get_companies_by_date_range(
                filter_start_date.strftime('%Y-%m-%d'),
                filter_end_date.strftime('%Y-%m-%d')
            )
            if date_results:
                st.success(f"üìÖ Found {len(date_results)} companies in date range")
                display_company_data(date_results, show_save_button=False, save_to_db=True)
            else:
                st.info("üìÖ No companies found in selected date range")
        else:
            st.error("‚ùå Start date must be before or equal to end date")

def show_settings():
    """Display the settings page."""
    st.header("‚öôÔ∏è Settings")
    
    # Database information
    st.markdown("### üíæ Database Information")
    total_companies = get_company_count()
    st.info(f"üìä Total companies in database: {total_companies}")
    
    # Supported sources
    st.markdown("### üåê Supported Sources")
    sources = get_supported_sources()
    for source, name in sources.items():
        st.write(f"‚úÖ {name}")
    
    # System information
    st.markdown("### üñ•Ô∏è System Information")
    st.write(f"üïí Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.write(f"üìÅ Working directory: {os.getcwd()}")
    
    # Clear data option
    st.markdown("### üóëÔ∏è Data Management")
    if st.button("üóëÔ∏è Clear All Data", type="secondary"):
        st.warning("‚ö†Ô∏è This will permanently delete all data!")
        if st.button("‚ö†Ô∏è Confirm Delete All Data", type="secondary"):
            if clear_all_companies():
                st.success("‚úÖ All data cleared successfully!")
                st.rerun()
            else:
                st.error("‚ùå Failed to clear data")

def show_natural_language_crawler():
    """Display the natural language crawler interface."""
    st.header("ü§ñ Natural Language Crawler")
    
    st.info("üí° **C√°ch s·ª≠ d·ª•ng**: Nh·∫≠p y√™u c·∫ßu b·∫±ng ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông hi·ªÉu v√† crawl d·ªØ li·ªáu!")
    
    # Example prompts
    with st.expander("üìù V√≠ d·ª• c√°c prompt c√≥ th·ªÉ s·ª≠ d·ª•ng"):
        st.markdown("""
        **Ti·∫øng Vi·ªát:**
        - "T√¥i mu·ªën l·∫•y tin t·ª´ vnexpress"
        - "Crawl d·ªØ li·ªáu t·ª´ trang techcrunch.com/startups"
        - "L·∫•y tin t·ª©c funding t·ª´ finsmes.com"
        - "T√¨m b√†i b√°o v·ªÅ startup funding tr√™n crunchbase"
        
        **English:**
        - "I want to crawl news from vnexpress"
        - "Get funding articles from techcrunch startups section"
        - "Extract data from finsmes.com"
        - "Find startup funding news on crunchbase"
        """)
    
    # Natural language input
    user_prompt = st.text_area(
        "Nh·∫≠p y√™u c·∫ßu c·ªßa b·∫°n:",
        placeholder="V√≠ d·ª•: T√¥i mu·ªën l·∫•y tin t·ª´ vnexpress ho·∫∑c Crawl d·ªØ li·ªáu t·ª´ techcrunch.com/startups",
        height=100
    )
    
    # Advanced options
    with st.expander("‚öôÔ∏è T√πy ch·ªçn n√¢ng cao"):
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("T·ª´ ng√†y:", value=None)
        with col2:
            end_date = st.date_input("ƒê·∫øn ng√†y:", value=None)
        
        save_to_db = st.checkbox("üíæ T·ª± ƒë·ªông l∆∞u v√†o Database", value=False, help="N·∫øu b·ªè ch·ªçn, b·∫°n s·∫Ω c√≥ th·ªÉ xem x√©t d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u")
        max_articles = st.slider("S·ªë b√†i b√°o t·ªëi ƒëa:", min_value=5, max_value=50, value=20)
    
    if st.button("üöÄ Crawl theo y√™u c·∫ßu", type="primary"):
        if user_prompt:
            with st.spinner("ƒêang ph√¢n t√≠ch y√™u c·∫ßu v√† crawl d·ªØ li·ªáu..."):
                try:
                    # Parse natural language prompt
                    parsed_url = parse_natural_language_prompt(user_prompt)
                    
                    if parsed_url:
                        st.success(f"‚úÖ ƒê√£ hi·ªÉu y√™u c·∫ßu: {parsed_url}")
                        
                        # Crawl the parsed URL
                        results = asyncio.run(crawl_list_page_async(
                            parsed_url, max_articles, 5, save_to_db,
                            start_date.strftime('%Y-%m-%d') if start_date else None,
                            end_date.strftime('%Y-%m-%d') if end_date else None
                        ))
                        
                        if results:
                            st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {len(results)} b√†i b√°o!")
                            
                            # Display results summary
                            successful = [r for r in results if r.get('success')]
                            st.info(f"üìä K·∫øt qu·∫£: {len(successful)} th√†nh c√¥ng / {len(results)} t·ªïng c·ªông")
                            
                            # Display results in table format
                            if successful:
                                table_data = []
                                for result in successful:
                                    table_data.append({
                                        'raised_date': result.get('raised_date'),
                                        'company_name': result.get('company_name'),
                                        'industry': result.get('industry'),
                                        'ceo_name': result.get('ceo_name'),
                                        'procurement_name': result.get('procurement_name'),
                                        'purchasing_name': result.get('purchasing_name'),
                                        'manager_name': result.get('manager_name'),
                                        'amount_raised': result.get('amount_raised'),
                                        'funding_round': result.get('funding_round'),
                                        'source': result.get('source'),
                                        'website': result.get('website'),
                                        'linkedin': result.get('linkedin'),
                                        'article_url': result.get('article_url')
                                    })
                                
                                st.success(f"üìä Hi·ªÉn th·ªã {len(table_data)} k·∫øt qu·∫£ th√†nh c√¥ng:")
                                display_company_data(table_data, show_save_button=not save_to_db, save_to_db=save_to_db)
                            
                            # Show failed results
                            failed_results = [r for r in results if not r.get('success')]
                            if failed_results:
                                with st.expander(f"‚ö†Ô∏è {len(failed_results)} k·∫øt qu·∫£ th·∫•t b·∫°i"):
                                    for i, result in enumerate(failed_results):
                                        st.error(f"**{i+1}. {result.get('url', 'URL kh√¥ng x√°c ƒë·ªãnh')}**: {result.get('error')}")
                            
                            # Auto-refresh cache
                            if save_to_db and successful:
                                st.info("üîÑ ƒêang c·∫≠p nh·∫≠t d·ªØ li·ªáu...")
                                st.cache_data.clear()
                                st.rerun()
                        else:
                            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i b√°o n√†o")
                    else:
                        st.error("‚ùå Kh√¥ng th·ªÉ hi·ªÉu y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i v·ªõi prompt r√µ r√†ng h∆°n.")
                        
                except Exception as e:
                    st.error(f"‚ùå L·ªói: {str(e)}")
        else:
            st.warning("Vui l√≤ng nh·∫≠p y√™u c·∫ßu c·ªßa b·∫°n")

def parse_natural_language_prompt(prompt):
    """Parse natural language prompt to extract URL and parameters."""
    prompt_lower = prompt.lower()
    
    # Common Vietnamese patterns
    vietnamese_patterns = [
        r't·ª´\s+(https?://[^\s]+)',
        r't·ª´\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        r'vnexpress',
        r'techcrunch',
        r'finsmes',
        r'crunchbase',
        r'startup',
        r'funding',
        r'raise fund',
        r'g·ªçi v·ªën',
        r'tin t·ª©c',
        r'b√°o',
        r'trang web',
        r'website'
    ]
    
    # Common English patterns
    english_patterns = [
        r'from\s+(https?://[^\s]+)',
        r'from\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        r'vnexpress',
        r'techcrunch',
        r'finsmes',
        r'crunchbase',
        r'startup',
        r'funding',
        r'news',
        r'website'
    ]
    
    # Check for direct URLs
    import re
    url_match = re.search(r'https?://[^\s]+', prompt)
    if url_match:
        return url_match.group(0)
    
    # Check for domain names
    domain_match = re.search(r'([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', prompt)
    if domain_match:
        domain = domain_match.group(1)
        # Add protocol if missing
        if not domain.startswith(('http://', 'https://')):
            return f"https://{domain}"
    
    # Map common keywords to URLs with better Vietnamese support
    keyword_mapping = {
        'vnexpress': 'https://vnexpress.net',
        'techcrunch': 'https://techcrunch.com/startups/',
        'finsmes': 'https://finsmes.com/',
        'crunchbase': 'https://news.crunchbase.com/sections/fintech-ecommerce/',
        'startup': 'https://techcrunch.com/startups/',
        'funding': 'https://finsmes.com/',
        'raise fund': 'https://finsmes.com/',
        'g·ªçi v·ªën': 'https://finsmes.com/',
        'tin t·ª©c': 'https://vnexpress.net',
        'b√°o': 'https://vnexpress.net',
        'trang web': None,  # Will be handled by domain detection
        'website': None,    # Will be handled by domain detection
        'news': 'https://techcrunch.com/startups/'
    }
    
    # Check for Vietnamese funding-related keywords
    funding_keywords = ['raise fund', 'g·ªçi v·ªën', 'funding', 'startup']
    for keyword in funding_keywords:
        if keyword in prompt_lower:
            # Prefer funding-specific sources
            if 'vnexpress' in prompt_lower:
                return 'https://vnexpress.net'
            elif 'techcrunch' in prompt_lower:
                return 'https://techcrunch.com/startups/'
            else:
                return 'https://finsmes.com/'
    
    # Check for general news keywords
    news_keywords = ['tin t·ª©c', 'b√°o', 'news']
    for keyword in news_keywords:
        if keyword in prompt_lower:
            if 'vnexpress' in prompt_lower:
                return 'https://vnexpress.net'
            elif 'techcrunch' in prompt_lower:
                return 'https://techcrunch.com/startups/'
            else:
                return 'https://vnexpress.net'
    
    # Check for specific website names
    for keyword, url in keyword_mapping.items():
        if keyword in prompt_lower and url:
            return url
    
    # If no specific match, try to extract any domain-like pattern
    domain_pattern = re.search(r'([a-zA-Z0-9-]+\.(com|net|org|vn|co|io))', prompt)
    if domain_pattern:
        domain = domain_pattern.group(1)
        return f"https://{domain}"
    
    return None

def show_ai_auto_discovery():
    """Display the AI Auto-Discovery interface."""
    st.header("ü§ñ AI Auto-Discovery")
    
    st.info("üöÄ **T√≠nh nƒÉng m·ªõi**: AI s·∫Ω t·ª± ƒë·ªông ph√¢n t√≠ch v√† crawl b·∫•t k·ª≥ trang web n√†o m√† kh√¥ng c·∫ßn c·∫•u h√¨nh tr∆∞·ªõc!")
    
    # Features explanation
    with st.expander("‚ú® T√≠nh nƒÉng AI Auto-Discovery"):
        st.markdown("""
        **ü§ñ AI ph√¢n t√≠ch website:**
        - T·ª± ƒë·ªông hi·ªÉu c·∫•u tr√∫c website
        - X√°c ƒë·ªãnh lo·∫°i website (news, blog, e-commerce...)
        - T√¨m chi·∫øn l∆∞·ª£c crawl t·ªëi ∆∞u
        
        **üîç T·ª± ƒë·ªông ph√°t hi·ªán:**
        - T√¨m navigation links
        - Ph√°t hi·ªán article patterns
        - X√°c ƒë·ªãnh content selectors
        
        **üì∞ Crawl th√¥ng minh:**
        - S·ª≠ d·ª•ng sitemap n·∫øu c√≥
        - Crawl category pages
        - Fallback v·ªÅ generic strategy
        
        **üéØ Kh√¥ng c·∫ßn c·∫•u h√¨nh:**
        - Kh√¥ng c·∫ßn th√™m v√†o sources.json
        - Kh√¥ng c·∫ßn vi·∫øt code m·ªõi
        - Ho·∫°t ƒë·ªông v·ªõi m·ªçi website
        """)
    
    # Input section with smart detection
    st.markdown("### üåê Nh·∫≠p th√¥ng tin website")
    
    # Input method selection
    input_method = st.radio(
        "Ch·ªçn c√°ch nh·∫≠p:",
        ["üîó URL tr·ª±c ti·∫øp", "ü§ñ Prompt t·ª± nhi√™n"],
        horizontal=True
    )
    
    if input_method == "üîó URL tr·ª±c ti·∫øp":
        website_input = st.text_input(
            "Nh·∫≠p URL website:",
            placeholder="https://example.com ho·∫∑c https://news.example.com",
            help="Nh·∫≠p URL b·∫•t k·ª≥ website n√†o b·∫°n mu·ªën crawl"
        )
        
        # Validate URL
        if website_input:
            if not is_valid_url(website_input):
                st.error("‚ùå URL kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p URL ƒë√∫ng ƒë·ªãnh d·∫°ng (v√≠ d·ª•: https://example.com)")
                website_input = None
            else:
                st.success(f"‚úÖ URL h·ª£p l·ªá: {website_input}")
    
    else:  # Natural language prompt
        website_input = st.text_area(
            "Nh·∫≠p y√™u c·∫ßu b·∫±ng ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh:",
            placeholder="V√≠ d·ª•: T√¥i mu·ªën l·∫•y tin t·ª´ vnexpress ho·∫∑c Crawl d·ªØ li·ªáu t·ª´ techcrunch.com/startups",
            height=100,
            help="Nh·∫≠p y√™u c·∫ßu t·ª± nhi√™n, AI s·∫Ω t·ª± ƒë·ªông hi·ªÉu v√† t√¨m website ph√π h·ª£p"
        )
        
        if website_input:
            # Parse natural language to URL
            parsed_url = parse_natural_language_prompt(website_input)
            if parsed_url:
                st.success(f"‚úÖ AI ƒë√£ hi·ªÉu y√™u c·∫ßu: {parsed_url}")
                website_input = parsed_url
            else:
                st.warning("‚ö†Ô∏è AI kh√¥ng th·ªÉ hi·ªÉu y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i v·ªõi prompt r√µ r√†ng h∆°n.")
                website_input = None
    
    # Advanced options
    with st.expander("‚öôÔ∏è T√πy ch·ªçn n√¢ng cao"):
        col1, col2, col3 = st.columns(3)
        with col1:
            max_articles = st.slider("S·ªë b√†i b√°o t·ªëi ƒëa:", min_value=5, max_value=50, value=20)
        with col2:
            save_to_db = st.checkbox("üíæ T·ª± ƒë·ªông l∆∞u v√†o Database", value=False, help="N·∫øu b·ªè ch·ªçn, b·∫°n s·∫Ω c√≥ th·ªÉ xem x√©t d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u")
        with col3:
            auto_process = st.checkbox("üß† T·ª± ƒë·ªông ph√¢n t√≠ch n·ªôi dung", value=True, help="T·ª± ƒë·ªông tr√≠ch xu·∫•t th√¥ng tin funding")
    
    # Crawl button
    if st.button("üöÄ AI Auto-Discovery & Crawl", type="primary", disabled=not website_input):
        if website_input:
            with st.spinner("ü§ñ AI ƒëang ph√¢n t√≠ch website..."):
                try:
                    # Import AI Auto-Discovery
                    from ai_auto_discovery import auto_crawl_website_async
                    
                    # Step 1: Analyze website
                    st.info("üîç ƒêang ph√¢n t√≠ch c·∫•u tr√∫c website...")
                    
                    # Step 2: Auto-crawl
                    results = asyncio.run(auto_crawl_website_async(website_input, max_articles))
                    
                    if results and not results[0].get('error'):
                        st.success(f"‚úÖ AI ƒë√£ crawl th√†nh c√¥ng {len(results)} b√†i b√°o!")
                        
                        # Display results summary
                        successful = [r for r in results if r.get('success')]
                        st.info(f"üìä K·∫øt qu·∫£: {len(successful)} th√†nh c√¥ng / {len(results)} t·ªïng c·ªông")
                        
                        # Display results in table format
                        if successful:
                            if auto_process:
                                # Auto-process with LLM
                                with st.spinner("üß† AI ƒëang ph√¢n t√≠ch n·ªôi dung..."):
                                    processed_results = asyncio.run(process_auto_discovered_articles(successful))
                                    if processed_results:
                                        st.success("‚úÖ ƒê√£ ph√¢n t√≠ch n·ªôi dung th√†nh c√¥ng!")
                                        display_company_data(processed_results, show_save_button=not save_to_db, save_to_db=save_to_db)
                                        
                                        # Auto-save to database if requested
                                        if save_to_db and processed_results:
                                            from db import insert_many_companies
                                            num_inserted = insert_many_companies(processed_results)
                                            st.success(f"üíæ ƒê√£ t·ª± ƒë·ªông l∆∞u {num_inserted} b·∫£n ghi v√†o database!")
                                            
                                            # Auto-refresh cache
                                            st.info("üîÑ ƒêang c·∫≠p nh·∫≠t d·ªØ li·ªáu...")
                                            st.cache_data.clear()
                                            st.rerun()
                                    else:
                                        st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ ph√¢n t√≠ch n·ªôi dung. Hi·ªÉn th·ªã d·ªØ li·ªáu th√¥...")
                                        display_raw_results(successful)
                            else:
                                # Display raw results
                                display_raw_results(successful)
                                
                                # Manual process option
                                if st.button("üß† Ph√¢n t√≠ch n·ªôi dung v·ªõi AI"):
                                    with st.spinner("AI ƒëang ph√¢n t√≠ch n·ªôi dung..."):
                                        processed_results = asyncio.run(process_auto_discovered_articles(successful))
                                        if processed_results:
                                            st.success("‚úÖ ƒê√£ ph√¢n t√≠ch n·ªôi dung th√†nh c√¥ng!")
                                            display_company_data(processed_results)
                                            
                                            # Save to database if requested
                                            if save_to_db and processed_results:
                                                from db import insert_many_companies
                                                num_inserted = insert_many_companies(processed_results)
                                                st.success(f"üíæ ƒê√£ t·ª± ƒë·ªông l∆∞u {num_inserted} b·∫£n ghi v√†o database!")
                        
                        # Show failed results
                        failed_results = [r for r in results if not r.get('success')]
                        if failed_results:
                            with st.expander(f"‚ö†Ô∏è {len(failed_results)} k·∫øt qu·∫£ th·∫•t b·∫°i"):
                                for i, result in enumerate(failed_results):
                                    st.error(f"**{i+1}. {result.get('url', 'URL kh√¥ng x√°c ƒë·ªãnh')}**: {result.get('error')}")
                    else:
                        error_msg = results[0].get('error') if results else "Kh√¥ng c√≥ k·∫øt qu·∫£"
                        
                        # Check if it's a bot blocking error
                        if "ch·∫∑n bot" in error_msg.lower() or "bot blocked" in error_msg.lower() or "403" in error_msg:
                            st.error(f"üö´ **Bot b·ªã ch·∫∑n**: {error_msg}")
                            
                            # Show bot blocking solutions
                            with st.expander("üîß Gi·∫£i ph√°p kh·∫Øc ph·ª•c bot blocking"):
                                st.markdown("""
                                **Website ƒëang ch·∫∑n bot. Th·ª≠ c√°c gi·∫£i ph√°p sau:**
                                
                                üîÑ **1. Th·ª≠ l·∫°i sau:**
                                - ƒê·ª£i 5-10 ph√∫t r·ªìi th·ª≠ l·∫°i
                                - Website c√≥ th·ªÉ t·∫°m th·ªùi ch·∫∑n do qu√° nhi·ªÅu request
                                
                                üåê **2. S·ª≠ d·ª•ng VPN:**
                                - Thay ƒë·ªïi IP address
                                - S·ª≠ d·ª•ng VPN kh√°c nhau
                                
                                ‚è∞ **3. Gi·∫£m t·ªëc ƒë·ªô:**
                                - Gi·∫£m s·ªë b√†i b√°o t·ªëi ƒëa
                                - Th√™m delay gi·ªØa c√°c request
                                
                                üéØ **4. Th·ª≠ website kh√°c:**
                                - Website n√†y c√≥ th·ªÉ c√≥ ch√≠nh s√°ch ch·∫∑n bot nghi√™m ng·∫∑t
                                - Th·ª≠ ngu·ªìn tin t∆∞∆°ng t·ª± kh√°c
                                
                                üìß **5. Li√™n h·ªá admin:**
                                - N·∫øu c·∫ßn crawl th∆∞·ªùng xuy√™n, li√™n h·ªá website ƒë·ªÉ xin permission
                                """)
                        else:
                            st.error(f"‚ùå AI Auto-Discovery th·∫•t b·∫°i: {error_msg}")
                        
                        # Provide general suggestions
                        st.markdown("### üí° G·ª£i √Ω kh·∫Øc ph·ª•c:")
                        st.markdown("""
                        1. **Ki·ªÉm tra URL**: ƒê·∫£m b·∫£o URL ƒë√∫ng v√† website c√≥ th·ªÉ truy c·∫≠p
                        2. **Th·ª≠ URL kh√°c**: Th·ª≠ v·ªõi URL c·ª• th·ªÉ h∆°n (v√≠ d·ª•: /news thay v√¨ homepage)
                        3. **Ki·ªÉm tra k·∫øt n·ªëi**: ƒê·∫£m b·∫£o c√≥ k·∫øt n·ªëi internet ·ªïn ƒë·ªãnh
                        4. **Th·ª≠ l·∫°i sau**: M·ªôt s·ªë website c√≥ th·ªÉ t·∫°m th·ªùi kh√¥ng kh·∫£ d·ª•ng
                        5. **S·ª≠ d·ª•ng VPN**: N·∫øu b·ªã ch·∫∑n bot, th·ª≠ s·ª≠ d·ª•ng VPN
                        """)
                        
                except Exception as e:
                    st.error(f"‚ùå L·ªói: {str(e)}")
                    logger.error(f"AI Auto-Discovery error: {e}")
        else:
            st.warning("Vui l√≤ng nh·∫≠p URL website ho·∫∑c y√™u c·∫ßu h·ª£p l·ªá")

async def process_auto_discovered_articles(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Process auto-discovered articles with LLM to extract funding information"""
    from llm_utils import extract_structured_data_llm
    from utils.data_normalizer import normalize_company_name, normalize_currency_amount, normalize_funding_round
    
    processed_results = []
    
    for article in articles:
        try:
            content = article.get('content', '')
            if not content:
                continue
            
            # Use LLM to extract funding information
            extracted_data = await asyncio.to_thread(extract_structured_data_llm, content)
            
            if extracted_data and extracted_data.get('company_name'):
                # Normalize data
                company_name = normalize_company_name(extracted_data.get('company_name'))
                
                amount_raised = extracted_data.get('amount_raised')
                if amount_raised:
                    normalized_amount, _ = normalize_currency_amount(str(amount_raised))
                    amount_raised = normalized_amount
                
                funding_round = extracted_data.get('funding_round')
                if funding_round:
                    funding_round = normalize_funding_round(funding_round)
                
                processed_results.append({
                    'raised_date': article.get('published_date'),
                    'company_name': company_name,
                    'industry': extracted_data.get('industry'),
                    'ceo_name': extracted_data.get('ceo_name'),
                    'procurement_name': extracted_data.get('procurement_name'),
                    'purchasing_name': extracted_data.get('purchasing_name'),
                    'manager_name': extracted_data.get('manager_name'),
                    'amount_raised': amount_raised,
                    'funding_round': funding_round,
                    'source': article.get('source', 'Auto-Discovered'),
                    'website': 'N/A',
                    'linkedin': 'N/A',
                    'article_url': article.get('url')
                })
        
        except Exception as e:
            logger.warning(f"Failed to process article {article.get('url')}: {e}")
            continue
    
    return processed_results

def is_valid_url(url: str) -> bool:
    """Validate if input is a valid URL"""
    import re
    from urllib.parse import urlparse
    
    # Check if it's a valid URL format
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False
    
    # Additional regex check for common patterns
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    
    return bool(url_pattern.match(url))

def display_raw_results(results: List[Dict[str, Any]]):
    """Display raw crawled results in a simple format"""
    if not results:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã")
        return
    
    # Create a simple table
    data = []
    for result in results:
        data.append({
            'Title': result.get('title', 'N/A'),
            'URL': result.get('url', 'N/A'),
            'Date': result.get('published_date', 'N/A'),
            'Source': result.get('source', 'N/A'),
            'Content Length': len(result.get('content', ''))
        })
    
    df = pd.DataFrame(data)
    st.dataframe(df, use_container_width=True)
    
    # Show content preview
    if st.checkbox("üëÄ Xem n·ªôi dung m·∫´u"):
        for i, result in enumerate(results[:3]):  # Show first 3
            with st.expander(f"üìÑ {result.get('title', 'No title')}"):
                st.markdown(f"**URL:** {result.get('url')}")
                st.markdown(f"**Date:** {result.get('published_date')}")
                st.markdown(f"**Content:**")
                content = result.get('content', '')
                st.text(content[:500] + "..." if len(content) > 500 else content)

if __name__ == "__main__":
    main()